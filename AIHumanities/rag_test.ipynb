{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama gpt-oss:20b 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python 3.12.3사용중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep  7 11:59:40 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             31W /  250W |       1MiB /  40960MiB |     N/A      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "py 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]\n",
      "cuda? True\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!python -c \"import torch,platform,sys; print('py',sys.version); print('cuda?',torch.cuda.is_available())\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%###########                                                   32.8%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to render group...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      ">>> NVIDIA GPU installed.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 환경변수 설정 후 서버 실행\n",
    "!export OLLAMA_USE_GPU=1\n",
    "# 선택: 특정 GPU만 쓰려면 (예: 0번 GPU만)\n",
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Ollama up: 200 {'models': []}\n",
      "\n",
      "--- /tmp/ollama.log (tail) ---\n",
      "Couldn't find '/home/work/.ollama/id_ed25519'. Generating new private key.\n",
      "Your new public key is: \n",
      "\n",
      "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKg3rjQ0249Jy85HS79DUi3L3QlJN71c0Fv6CbmSj5Bl\n",
      "\n",
      "time=2025-09-07T12:04:09.349+09:00 level=INFO source=routes.go:1331 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/work/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NEW_ESTIMATES:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-09-07T12:04:09.349+09:00 level=INFO source=images.go:477 msg=\"total blobs: 0\"\n",
      "time=2025-09-07T12:04:09.349+09:00 level=INFO source=images.go:484 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-09-07T12:04:09.349+09:00 level=INFO source=routes.go:1384 msg=\"Listening on 127.0.0.1:11434 (version 0.11.10)\"\n",
      "time=2025-09-07T12:04:09.350+09:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
      "time=2025-09-07T12:04:09.794+09:00 level=INFO source=types.go:131 msg=\"inference compute\" id=GPU-3e598db6-9daf-4004-6afe-71cd19c1e433 library=cuda variant=v12 compute=8.0 driver=12.8 name=\"NVIDIA A100-PCIE-40GB\" total=\"39.5 GiB\" available=\"39.1 GiB\"\n",
      "[GIN] 2025/09/07 - 12:04:11 | 200 |     354.387µs |       127.0.0.1 | GET      \"/api/tags\"\n"
     ]
    }
   ],
   "source": [
    "# ✅ Jupyter에서 Ollama 서버를 GPU 모드로 '비차단' 실행하기\n",
    "import os, subprocess, time, signal, requests\n",
    "\n",
    "# 1) 환경변수: GPU 사용\n",
    "env = os.environ.copy()\n",
    "env[\"OLLAMA_USE_GPU\"] = \"1\"          # 서버 프로세스에 적용됨\n",
    "env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"    # 필요시 GPU 인덱스 지정\n",
    "\n",
    "# 2) 로그 파일\n",
    "LOG = \"/tmp/ollama.log\"\n",
    "logf = open(LOG, \"ab\", buffering=0)\n",
    "\n",
    "# 3) 기존 떠 있는 ollama serve가 있으면 정리(선택)\n",
    "try:\n",
    "    subprocess.run([\"pkill\", \"-f\", \"ollama serve\"], check=False)\n",
    "    time.sleep(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 4) 비차단 실행 (주피터에선 & 금지 → Popen으로 대체)\n",
    "#    preexec_fn=os.setpgrp 로 자식 프로세스 그룹을 분리(노트북 중지와 분리)\n",
    "proc = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"],\n",
    "    stdout=logf, stderr=subprocess.STDOUT,\n",
    "    env=env,\n",
    "    preexec_fn=os.setpgrp  # 리눅스/WSL2\n",
    ")\n",
    "\n",
    "# 5) 헬스체크\n",
    "time.sleep(2)\n",
    "try:\n",
    "    r = requests.get(\"http://127.0.0.1:11434/api/tags\", timeout=3)\n",
    "    print(\"✔️ Ollama up:\", r.status_code, r.json())\n",
    "except Exception as e:\n",
    "    print(\"❌ 서버 확인 실패:\", e)\n",
    "\n",
    "# 6) 최근 로그 몇 줄 보기\n",
    "try:\n",
    "    with open(LOG, \"rb\") as f:\n",
    "        tail = f.read().splitlines()[-20:]\n",
    "    print(\"\\n--- /tmp/ollama.log (tail) ---\")\n",
    "    for line in tail:\n",
    "        try:\n",
    "            print(line.decode(\"utf-8\", \"ignore\"))\n",
    "        except:\n",
    "            print(line)\n",
    "except FileNotFoundError:\n",
    "    print(\"로그가 아직 생성되지 않았어요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ollama server 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pkill', '-f', 'ollama serve'], returncode=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"pkill\", \"-f\", \"ollama serve\"], check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ollama serve 특정 모델 위치에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['ollama', 'serve']>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find '/home/work/.ollama/id_ed25519'. Generating new private key.\n",
      "Your new public key is: \n",
      "\n",
      "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIFaxEXINuJSeK/YZNJET7jm45Muu9reP5getxQ09UoXW\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-09-07T12:35:43.035+09:00 level=INFO source=routes.go:1331 msg=\"server config\" env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:./models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NEW_ESTIMATES:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2025-09-07T12:35:43.039+09:00 level=INFO source=images.go:477 msg=\"total blobs: 5\"\n",
      "time=2025-09-07T12:35:43.040+09:00 level=INFO source=images.go:484 msg=\"total unused blobs removed: 0\"\n",
      "time=2025-09-07T12:35:43.042+09:00 level=INFO source=routes.go:1384 msg=\"Listening on 127.0.0.1:11434 (version 0.11.10)\"\n",
      "time=2025-09-07T12:35:43.042+09:00 level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\n",
      "time=2025-09-07T12:35:43.491+09:00 level=INFO source=types.go:131 msg=\"inference compute\" id=GPU-3e598db6-9daf-4004-6afe-71cd19c1e433 library=cuda variant=v12 compute=8.0 driver=12.8 name=\"NVIDIA A100-PCIE-40GB\" total=\"39.5 GiB\" available=\"39.1 GiB\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:35:48 | 200 |      49.897µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2025/09/07 - 12:35:48 | 200 |    2.577167ms |       127.0.0.1 | GET      \"/api/tags\"\n",
      "[GIN] 2025/09/07 - 12:35:50 | 200 |      25.541µs |       127.0.0.1 | HEAD     \"/\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-09-07T12:35:52.389+09:00 level=INFO source=download.go:177 msg=\"downloading aeda25e63ebd in 16 208 MB part(s)\"\n",
      "time=2025-09-07T12:36:00.143+09:00 level=INFO source=download.go:177 msg=\"downloading e0a42594d802 in 1 358 B part(s)\"\n",
      "time=2025-09-07T12:36:01.642+09:00 level=INFO source=download.go:177 msg=\"downloading dd084c7d92a3 in 1 8.4 KB part(s)\"\n",
      "time=2025-09-07T12:36:03.145+09:00 level=INFO source=download.go:177 msg=\"downloading 3116c5225075 in 1 77 B part(s)\"\n",
      "time=2025-09-07T12:36:04.629+09:00 level=INFO source=download.go:177 msg=\"downloading b6ae5839783f in 1 489 B part(s)\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:36:32 | 200 | 42.076637056s |       127.0.0.1 | POST     \"/api/pull\"\n"
     ]
    }
   ],
   "source": [
    "env = os.environ.copy()\n",
    "env[\"OLLAMA_MODELS\"] = \"./models\"\n",
    "subprocess.Popen([\"ollama\", \"serve\"], env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ollama - gpt-oss:20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:09:28 | 200 |      60.501µs |       127.0.0.1 | HEAD     \"/\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:09:29 | 200 |   813.84471ms |       127.0.0.1 | POST     \"/api/pull\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling b112e727c6f1: 100% ▕██████████████████▏  13 GB                         \u001b[K\n",
      "pulling fa6710a93d78: 100% ▕██████████████████▏ 7.2 KB                         \u001b[K\n",
      "pulling f60356777647: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling d8ba2f9a17b3: 100% ▕██████████████████▏   18 B                         \u001b[K\n",
      "pulling 55c108d8e936: 100% ▕██████████████████▏  489 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ollama', 'pull', 'gpt-oss:20b'], returncode=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, subprocess\n",
    "\n",
    "# 원하는 경로 지정\n",
    "os.environ[\"OLLAMA_MODELS\"] = \"./models\"\n",
    "\n",
    "# 디렉토리가 없으면 먼저 생성\n",
    "os.makedirs(os.environ[\"OLLAMA_MODELS\"], exist_ok=True)\n",
    "\n",
    "# 모델 풀\n",
    "subprocess.run([\"ollama\", \"pull\", \"gpt-oss:20b\"], check=True, env=os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:13:58 | 200 |      25.255µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2025/09/07 - 12:13:58 | 200 |      4.7796ms |       127.0.0.1 | GET      \"/api/tags\"\n",
      "NAME           ID              SIZE     MODIFIED      \n",
      "gpt-oss:20b    aa4295ac10c3    13 GB    4 minutes ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/IROLIM/AIHumanities\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/bitsandbytes-0.45.4.dev0-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-0.3.7-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.25-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.10.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.4.36)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
      "Collecting ollama<1.0.0,>=0.5.3 (from langchain-ollama)\n",
      "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (23.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.10.15)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.24.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.2.0)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_ollama-0.3.7-py3-none-any.whl (24 kB)\n",
      "Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
      "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
      "Downloading langsmith-0.4.25-py3-none-any.whl (379 kB)\n",
      "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading zstandard-0.24.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, tenacity, jsonpatch, requests-toolbelt, ollama, langsmith, langchain-core, langchain-text-splitters, langchain-ollama, langchain\n",
      "Successfully installed jsonpatch-1.33 langchain-0.3.27 langchain-core-0.3.75 langchain-ollama-0.3.7 langchain-text-splitters-0.3.11 langsmith-0.4.25 ollama-0.5.3 requests-toolbelt-1.0.0 tenacity-9.1.2 zstandard-0.24.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-09-07T12:10:01.931+09:00 level=INFO source=server.go:199 msg=\"model wants flash attention\"\n",
      "time=2025-09-07T12:10:01.932+09:00 level=INFO source=server.go:216 msg=\"enabling flash attention\"\n",
      "time=2025-09-07T12:10:01.932+09:00 level=WARN source=server.go:224 msg=\"kv cache type not supported by model\" type=\"\"\n",
      "time=2025-09-07T12:10:01.932+09:00 level=INFO source=server.go:398 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --port 40953\"\n",
      "time=2025-09-07T12:10:01.959+09:00 level=INFO source=runner.go:1251 msg=\"starting ollama engine\"\n",
      "time=2025-09-07T12:10:01.959+09:00 level=INFO source=runner.go:1286 msg=\"Server listening on 127.0.0.1:40953\"\n",
      "time=2025-09-07T12:10:02.235+09:00 level=INFO source=server.go:503 msg=\"system memory\" total=\"1007.1 GiB\" free=\"925.8 GiB\" free_swap=\"8.0 GiB\"\n",
      "time=2025-09-07T12:10:02.520+09:00 level=INFO source=memory.go:36 msg=\"new model will fit in available VRAM across minimum required GPUs, loading\" model=models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 library=cuda parallel=1 required=\"13.1 GiB\" gpus=1\n",
      "time=2025-09-07T12:10:02.813+09:00 level=INFO source=server.go:543 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split=[25] memory.available=\"[39.1 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"13.1 GiB\" memory.required.partial=\"13.1 GiB\" memory.required.kv=\"300.0 MiB\" memory.required.allocations=\"[13.1 GiB]\" memory.weights.total=\"11.7 GiB\" memory.weights.repeating=\"10.7 GiB\" memory.weights.nonrepeating=\"1.1 GiB\" memory.graph.full=\"122.0 MiB\" memory.graph.partial=\"122.0 MiB\"\n",
      "time=2025-09-07T12:10:02.814+09:00 level=INFO source=runner.go:1170 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:40 GPULayers:25[ID:GPU-3e598db6-9daf-4004-6afe-71cd19c1e433 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-09-07T12:10:02.933+09:00 level=INFO source=ggml.go:131 msg=\"\" architecture=gptoss file_type=MXFP4 name=\"\" description=\"\" num_tensors=315 num_key_values=30\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-PCIE-40GB, compute capability 8.0, VMM: yes, ID: GPU-3e598db6-9daf-4004-6afe-71cd19c1e433\n",
      "load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "time=2025-09-07T12:10:03.239+09:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-09-07T12:10:03.427+09:00 level=INFO source=ggml.go:487 msg=\"offloading 24 repeating layers to GPU\"\n",
      "time=2025-09-07T12:10:03.427+09:00 level=INFO source=ggml.go:493 msg=\"offloading output layer to GPU\"\n",
      "time=2025-09-07T12:10:03.427+09:00 level=INFO source=ggml.go:498 msg=\"offloaded 25/25 layers to GPU\"\n",
      "time=2025-09-07T12:10:03.428+09:00 level=INFO source=backend.go:310 msg=\"model weights\" device=CUDA0 size=\"11.8 GiB\"\n",
      "time=2025-09-07T12:10:03.428+09:00 level=INFO source=backend.go:315 msg=\"model weights\" device=CPU size=\"1.1 GiB\"\n",
      "time=2025-09-07T12:10:03.428+09:00 level=INFO source=backend.go:321 msg=\"kv cache\" device=CUDA0 size=\"300.0 MiB\"\n",
      "time=2025-09-07T12:10:03.428+09:00 level=INFO source=backend.go:332 msg=\"compute graph\" device=CUDA0 size=\"121.8 MiB\"\n",
      "time=2025-09-07T12:10:03.428+09:00 level=INFO source=backend.go:337 msg=\"compute graph\" device=CPU size=\"5.6 MiB\"\n",
      "time=2025-09-07T12:10:03.428+09:00 level=INFO source=backend.go:342 msg=\"total memory\" size=\"13.3 GiB\"\n",
      "time=2025-09-07T12:10:03.428+09:00 level=INFO source=sched.go:473 msg=\"loaded runners\" count=1\n",
      "time=2025-09-07T12:10:03.428+09:00 level=INFO source=server.go:1250 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-09-07T12:10:03.428+09:00 level=INFO source=server.go:1284 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2025-09-07T12:10:33.812+09:00 level=INFO source=server.go:1288 msg=\"llama runner started in 31.88 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:10:37 | 200 | 37.108818955s |       127.0.0.1 | POST     \"/api/generate\"\n",
      "PyTorch를 사용한다면 `torch.cuda.is_available()`가 **True**이고, 모델의 파라미터가 `next(model.parameters()).device.type == 'cuda'`인지 한 줄로 확인하면 GPU로 추론 중인지 바로 판별할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "resp = requests.post(\n",
    "    \"http://127.0.0.1:11434/api/generate\",\n",
    "    json={\"model\": \"gpt-oss:20b\", \"prompt\": \"GPU로 추론 중인지 확인하는 간단한 방법을 한 문장으로 말해줘(한국어).\", \"stream\": False},\n",
    "    timeout=120,\n",
    ")\n",
    "print(resp.json()[\"response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-09-07T12:17:57.045+09:00 level=INFO source=server.go:199 msg=\"model wants flash attention\"\n",
      "time=2025-09-07T12:17:57.045+09:00 level=INFO source=server.go:216 msg=\"enabling flash attention\"\n",
      "time=2025-09-07T12:17:57.045+09:00 level=WARN source=server.go:224 msg=\"kv cache type not supported by model\" type=\"\"\n",
      "time=2025-09-07T12:17:57.045+09:00 level=INFO source=server.go:398 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 --port 44263\"\n",
      "time=2025-09-07T12:17:57.072+09:00 level=INFO source=runner.go:1251 msg=\"starting ollama engine\"\n",
      "time=2025-09-07T12:17:57.072+09:00 level=INFO source=runner.go:1286 msg=\"Server listening on 127.0.0.1:44263\"\n",
      "time=2025-09-07T12:17:57.299+09:00 level=INFO source=server.go:503 msg=\"system memory\" total=\"1007.1 GiB\" free=\"928.9 GiB\" free_swap=\"8.0 GiB\"\n",
      "time=2025-09-07T12:17:57.549+09:00 level=INFO source=memory.go:36 msg=\"new model will fit in available VRAM across minimum required GPUs, loading\" model=models/blobs/sha256-b112e727c6f18875636c56a779790a590d705aec9e1c0eb5a97d51fc2a778583 library=cuda parallel=1 required=\"13.1 GiB\" gpus=1\n",
      "time=2025-09-07T12:17:57.790+09:00 level=INFO source=server.go:543 msg=offload library=cuda layers.requested=-1 layers.model=25 layers.offload=25 layers.split=[25] memory.available=\"[39.1 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"13.1 GiB\" memory.required.partial=\"13.1 GiB\" memory.required.kv=\"300.0 MiB\" memory.required.allocations=\"[13.1 GiB]\" memory.weights.total=\"11.7 GiB\" memory.weights.repeating=\"10.7 GiB\" memory.weights.nonrepeating=\"1.1 GiB\" memory.graph.full=\"122.0 MiB\" memory.graph.partial=\"122.0 MiB\"\n",
      "time=2025-09-07T12:17:57.792+09:00 level=INFO source=runner.go:1170 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:8192 KvCacheType: NumThreads:40 GPULayers:25[ID:GPU-3e598db6-9daf-4004-6afe-71cd19c1e433 Layers:25(0..24)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-09-07T12:17:57.930+09:00 level=INFO source=ggml.go:131 msg=\"\" architecture=gptoss file_type=MXFP4 name=\"\" description=\"\" num_tensors=315 num_key_values=30\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-PCIE-40GB, compute capability 8.0, VMM: yes, ID: GPU-3e598db6-9daf-4004-6afe-71cd19c1e433\n",
      "load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "time=2025-09-07T12:17:58.052+09:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-09-07T12:17:58.213+09:00 level=INFO source=ggml.go:487 msg=\"offloading 24 repeating layers to GPU\"\n",
      "time=2025-09-07T12:17:58.213+09:00 level=INFO source=ggml.go:493 msg=\"offloading output layer to GPU\"\n",
      "time=2025-09-07T12:17:58.213+09:00 level=INFO source=ggml.go:498 msg=\"offloaded 25/25 layers to GPU\"\n",
      "time=2025-09-07T12:17:58.214+09:00 level=INFO source=backend.go:310 msg=\"model weights\" device=CUDA0 size=\"11.8 GiB\"\n",
      "time=2025-09-07T12:17:58.214+09:00 level=INFO source=backend.go:315 msg=\"model weights\" device=CPU size=\"1.1 GiB\"\n",
      "time=2025-09-07T12:17:58.214+09:00 level=INFO source=backend.go:321 msg=\"kv cache\" device=CUDA0 size=\"300.0 MiB\"\n",
      "time=2025-09-07T12:17:58.214+09:00 level=INFO source=backend.go:332 msg=\"compute graph\" device=CUDA0 size=\"121.8 MiB\"\n",
      "time=2025-09-07T12:17:58.214+09:00 level=INFO source=backend.go:337 msg=\"compute graph\" device=CPU size=\"5.6 MiB\"\n",
      "time=2025-09-07T12:17:58.214+09:00 level=INFO source=backend.go:342 msg=\"total memory\" size=\"13.3 GiB\"\n",
      "time=2025-09-07T12:17:58.214+09:00 level=INFO source=sched.go:473 msg=\"loaded runners\" count=1\n",
      "time=2025-09-07T12:17:58.214+09:00 level=INFO source=server.go:1250 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-09-07T12:17:58.214+09:00 level=INFO source=server.go:1284 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2025-09-07T12:18:01.977+09:00 level=INFO source=server.go:1288 msg=\"llama runner started in 4.93 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:18:03 | 200 |  8.146732935s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "저는 LangChain과 Ollama를 통해 실행되는 AI 언어 모델로, 여러분의 질문에 답하고 도움을 드립니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Ollama 연결\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://127.0.0.1:11434\",  # ollama serve가 켜져 있는 주소\n",
    "    model=\"gpt-oss:20b\",\n",
    "    temperature=0.7,\n",
    "    num_ctx=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:21:25 | 200 | 40.488654103s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "## 장곡사 미륵불 괘불탱이란?\n",
      "\n",
      "### 1. 장곡사(장곡사)란?\n",
      "- **위치**: 경상북도 경주(전통적으로 경주성 안에 위치한 작은 사찰)  \n",
      "- **역사**: 조선시대에 설립되었으며, 사찰 자체는 규모가 작지만, 그 안에 보존된 미륵불은 한국 불교 문화의 보물로 평가됩니다.  \n",
      "- **문화유산**: 장곡사는 ‘경주 사적지구’에 포함되어 유네스코 세계문화유산으로 지정된 경주 지역의 한 축을 담당합니다.\n",
      "\n",
      "### 2. 미륵불 괘불탱(미륵불 괘불탱) 구조\n",
      "| 항목 | 내용 |\n",
      "|------|------|\n",
      "| **형태** | 직사각형 구조의 소형 사당(‘탱’은 ‘탱’형 사당’을 뜻함) |\n",
      "| **건축 시기** | 12세기 후반, 고려시대(1195년경) |\n",
      "| **특징** | 기와(瓦) 지붕, 석판(石板) 기반, 내부는 한 개의 큰 동상만을 배치한 단순함이 특징 |\n",
      "| **용도** | 미륵불을 모시고 예불·공덕을 쌓는 공간 |\n",
      "\n",
      "### 3. 미륵불(미륵불) 동상\n",
      "| 항목 | 내용 |\n",
      "|------|------|\n",
      "| **재료** | 청동(동) |\n",
      "| **크기** | 약 4.5m (높이) |\n",
      "| **무게** | 약 5톤 |\n",
      "| **제작 시기** | 1184년(고려 왕경종 3년) |\n",
      "| **제작자** | 알려지지 않음(전통적으로 ‘고려 청동조각가’라 전해짐) |\n",
      "| **특징** | <br>1. **손 모양**: 오른손은 ‘무릉’ 모양(종교적 인형)으로 향해 있고, 왼손은 ‘삼지’ 모양으로 가라앉혀 있다.<br>2. **표정**: 고요하고 온화한 미소, 눈은 살짝 감겨 있는 형태.<br>3. **의복**: 금박이 발달한 장식이 돋보이는 긴 옷, 머리카락은 긴 물결 모양.<br>4. **꽃**: 오른손에 꽃(연꽃)을 들고 있는 모습. |\n",
      "| **예술적 의의** | 고려 청동 조각의 정점이라 불리며, ‘부드러운 곡선’과 ‘자연스러운 비율’이 조화롭게 표현된 대표작. |\n",
      "| **문화적 의미** | 미륵(미래의 부처) 불상은 ‘미래에 다가올 구원’을 상징, 한국 불교에서 희망과 구원의 상징으로 여겨짐. |\n",
      "\n",
      "### 4. 국가문화재 지정\n",
      "- **문화재 번호**: 114번 (장곡사 미륵불 괘불탱)\n",
      "- **지정 시기**: 1962년(제1회 문화재 보존 및 관리 법령 개정 시)\n",
      "- **보호 대상**: 동상과 사당 전체가 국가문화재로 지정되어 보존·관리되고 있음.\n",
      "\n",
      "### 5. 현재 상황 및 활용\n",
      "- **관람**: 장곡사 미륵불 괘불탱은 사찰 내부에 위치해 있어, 사찰 방문객은 물론 관광객이 방문할 수 있다.  \n",
      "- **공예·연구**: 청동 조각 연구, 고려 예술 연구에 중요한 자료로 활용되고 있다.  \n",
      "- **문화행사**: 정기적으로 불교 예불과 문화행사가 열리며, 미륵불을 중심으로 한 ‘미륵불제’ 같은 전통 의식이 진행된다.  \n",
      "- **전시**: 국립중앙박물관, 경주박물관 등에서 가끔 전시되며, 국내외 연구자들이 방문한다.\n",
      "\n",
      "### 6. 미륵불 괘불탱이 가지는 의미\n",
      "| 관점 | 내용 |\n",
      "|------|------|\n",
      "| **종교적** | 미륵 불상은 ‘미래에 다가올 구원’과 ‘불교적 이상’의 상징으로, 신자들에게 희망과 소망을 심어준다. |\n",
      "| **문화적** | 고려청동의 예술적 정점으로서, 한국 전통 예술의 수준과 기술을 보여준다. |\n",
      "| **역사적** | 고려시대의 사찰 건축과 청동 조각 기술이 결합된 사례로, 역사 연구에 큰 가치를 지닌다. |\n",
      "| **교육적** | 청동 조각, 사찰 건축, 불교문화 등 다각적인 교육 자료로 활용된다. |\n",
      "\n",
      "---\n",
      "\n",
      "### 정리\n",
      "- **장곡사 미륵불 괘불탱**은 경주에 위치한 작은 사찰 장곡사 안에 있는 ‘미륵불’을 모신 사당(탱)이며, 12세기 고려시대에 건축된 구조와 1184년에 제작된 청동 미륵불 동상이 핵심이다.  \n",
      "- 미륵불은 4.5m 크기, 5톤 무게의 청동으로 조각되어 고려청동 예술의 정점을 이루며, ‘미래의 부처’라는 상징적 의미를 지닌다.  \n",
      "- 이 사당과 동상은 1962년 국가문화재(번호 114)로 지정되어 보존·관리되고 있으며, 경주 관광 및 불교 문화 연구의 중요한 명소가 되고 있다.  \n",
      "\n",
      "장곡사 미륵불 괘불탱은 단순히 예술작품을 넘어, 한국 불교의 역사와 문화, 그리고 미래에 대한 희망을 한눈에 보여주는 살아있는 ‘문화유산’이라고 할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 단일 메시지\n",
    "resp = llm.invoke(\"장곡사 미륵불 괘불탱에 대해서 설명해줘.\")\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:24:48 | 200 |  34.54837296s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "## 장곡사 미륵불 괘불탱이란?\n",
      "\n",
      "**장곡사(長谷寺)**는 경상북도 경주에 위치한 사찰로, 신라 시대(7세기) 초기에 세워졌습니다.  \n",
      "장곡사는 특히 ‘미륵불’과 ‘괘불탱’이라는 두 가지 대표적인 문화재가 있는 곳으로 유명합니다. 이 두 유물은 모두 **국보**로 지정되어 있으며, 신라의 석조미술과 불교 문화가 얼마나 정교하고 풍부했는지를 보여주는 귀중한 예시입니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. 미륵불 (미륵불상)\n",
      "\n",
      "| 항목 | 내용 |\n",
      "|------|------|\n",
      "| **형태** | 2.5m 높이의 석조 미륵불상 |\n",
      "| **재료** | 단일석(흑암석) |\n",
      "| **시대** | 신라 7세기 초 |\n",
      "| **특징** | ① **평온한 표정** – 미륵이 바라보는 눈빛이 부드럽고 온화함<br>② **복장과 장식** – 섬세하게 조각된 장식품(복장, 머리 장식, 팔찌 등)<br>③ **자연스러운 비율** – 신체 비례가 자연스럽고 균형 잡힘 |\n",
      "| **의의** | 미륵보살은 ‘미래의 부처’로서, 인간이 불교적 깨달음에 이르는 길을 상징합니다. 장곡사의 미륵불은 신라 시대의 불교 예술이 얼마나 정교했는지를 보여주는 대표작입니다. |\n",
      "| **문화재 등급** | 국보 제112호 |\n",
      "\n",
      "---\n",
      "\n",
      "### 2. 괘불탱 (Gwaebul‑tang)\n",
      "\n",
      "> **괘불탱**은 ‘괘불(蓮佛)’이라는 ‘연꽃 불상’이 들어 있는 작은 석조 탑을 말합니다.  \n",
      "> ‘괘불’은 ‘연꽃’이라는 뜻을 가지고 있어, 깨끗하고 순수한 깨달음을 상징합니다.\n",
      "\n",
      "| 항목 | 내용 |\n",
      "|------|------|\n",
      "| **형태** | 1.5m 정도의 작은 석조 탑(정사형) |\n",
      "| **구조** | ① **연꽃 모양의 기초** – 연꽃을 형상화한 기초부<br>② **정사형 본체** – 4개의 벽면이 직각으로 연결된 구조<br>③ **탑 꼭대기** – 작은 돔형 마루 |\n",
      "| **시대** | 신라 7세기 초 |\n",
      "| **특징** | ① **연꽃 기초** – 연꽃의 꽃잎이 섬세하게 조각<br>② **단순하지만 정교한 장식** – 벽면에 미세한 문양과 기하학적 패턴<br>③ **상징성** – 연꽃은 물 위에서 깨끗이 피어나는 상징으로, 불교에서 깨달음과 순수함을 상징 |\n",
      "| **의의** | 괘불탱은 신라 시대의 석조 건축과 불교 예술이 결합된 대표적인 예시입니다. 특히 연꽃 모양의 기초는 불교에서 매우 중요한 상징을 담고 있어, 신라 불교의 신비로운 면모를 보여줍니다. |\n",
      "| **문화재 등급** | 국보 제113호 |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. 장곡사의 문화적·역사적 가치\n",
      "\n",
      "1. **신라의 석조미술**  \n",
      "   - 미륵불과 괘불탱은 신라 시대 석조미술이 얼마나 정교하고 섬세했는지를 보여주는 대표작입니다.  \n",
      "   - 단일석으로 조각된 미륵불은 당시의 석공 기술과 예술적 감각을 반영합니다.\n",
      "\n",
      "2. **불교 사상과 예술의 결합**  \n",
      "   - 미륵불은 ‘미래의 부처’라는 불교 사상을 시각적으로 구현한 작품입니다.  \n",
      "   - 괘불탱의 연꽃 기초는 깨달음과 순수함을 상징하며, 불교의 핵심 가치를 시각적으로 전달합니다.\n",
      "\n",
      "3. **문화재 보존과 연구**  \n",
      "   - 두 유물은 국보로 지정되어 있어, 보존과 연구가 활발히 진행되고 있습니다.  \n",
      "   - 최근에는 3D 스캔, 재생성, 디지털 보존 기술을 활용해 유물의 상태를 정밀히 분석하고 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. 장곡사 방문 팁\n",
      "\n",
      "| 팁 | 내용 |\n",
      "|----|------|\n",
      "| **방문 시간** | 오전 9시~오후 5시 (입장료는 사찰에 따라 다를 수 있음) |\n",
      "| **가이드 투어** | 사찰 내부와 미륵불, 괘불탱을 자세히 안내해 주는 가이드 투어가 운영됩니다. |\n",
      "| **사진 촬영** | 미륵불과 괘불탱은 사진 촬영이 허용되지만, 조명과 조리개에 주의해 주세요. |\n",
      "| **문화 체험** | 사찰에서 진행되는 명상 체험이나 전통 차 시음 프로그램에 참여해 보세요. |\n",
      "\n",
      "---\n",
      "\n",
      "### 5. 마무리\n",
      "\n",
      "장곡사의 미륵불과 괘불탱은 신라 시대의 석조미술과 불교 사상을 한눈에 볼 수 있는 대표적인 문화유산입니다. 두 유물은 각각 국보로 지정되어 있어, 한국의 문화와 예술을 이해하는 데 큰 도움이 됩니다. 경주를 방문하신다면, 장곡사를 꼭 들러서 이 두 예술작품을 직접 감상해 보시길 권합니다.\n"
     ]
    }
   ],
   "source": [
    "# 3) 테스트 import\n",
    "\n",
    "# llm = ChatOllama(base_url=\"http://127.0.0.1:11434\", model=\"gpt-oss:20b\", temperature=0.3)\n",
    "resp = llm.invoke([\n",
    "    SystemMessage(content=\"너는 AI 문화유산 스마트도슨트야.\"),\n",
    "    HumanMessage(content=\"장곡사 미륵불 괘불탱에 대해서 설명해줘.\"),\n",
    "])\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ollama - gemma3:12b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:36:40 | 200 |      33.555µs |       127.0.0.1 | HEAD     \"/\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:36:41 | 200 |  746.920851ms |       127.0.0.1 | POST     \"/api/pull\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling aeda25e63ebd: 100% ▕██████████████████▏ 3.3 GB                         \u001b[K\n",
      "pulling e0a42594d802: 100% ▕██████████████████▏  358 B                         \u001b[K\n",
      "pulling dd084c7d92a3: 100% ▕██████████████████▏ 8.4 KB                         \u001b[K\n",
      "pulling 3116c5225075: 100% ▕██████████████████▏   77 B                         \u001b[K\n",
      "pulling b6ae5839783f: 100% ▕██████████████████▏  489 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['ollama', 'pull', 'gemma3'], returncode=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:36:52 | 200 |      26.533µs |       127.0.0.1 | HEAD     \"/\"\n",
      "[GIN] 2025/09/07 - 12:36:52 | 200 |    6.479818ms |       127.0.0.1 | GET      \"/api/tags\"\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess\n",
    "\n",
    "# 원하는 경로 지정 (여기선 ./models)\n",
    "os.environ[\"OLLAMA_MODELS\"] = \"./models\"\n",
    "\n",
    "# 디렉토리가 없으면 먼저 생성\n",
    "os.makedirs(os.environ[\"OLLAMA_MODELS\"], exist_ok=True)\n",
    "\n",
    "# gemma3:12b 모델 풀\n",
    "subprocess.run([\"ollama\", \"pull\", \"gemma3\"], check=True, env=os.environ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama 연결\n",
    "llm1 = ChatOllama(\n",
    "    base_url=\"http://127.0.0.1:11434\",  # ollama serve가 켜져 있는 주소\n",
    "    model=\"gemma3:latest\",\n",
    "    temperature=0.7,\n",
    "    num_ctx=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2025-09-07T12:38:21.125+09:00 level=INFO source=server.go:398 msg=\"starting runner\" cmd=\"/usr/local/bin/ollama runner --ollama-engine --model models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 --port 43343\"\n",
      "time=2025-09-07T12:38:21.150+09:00 level=INFO source=runner.go:1251 msg=\"starting ollama engine\"\n",
      "time=2025-09-07T12:38:21.151+09:00 level=INFO source=runner.go:1286 msg=\"Server listening on 127.0.0.1:43343\"\n",
      "time=2025-09-07T12:38:21.409+09:00 level=INFO source=server.go:503 msg=\"system memory\" total=\"1007.1 GiB\" free=\"926.1 GiB\" free_swap=\"8.0 GiB\"\n",
      "time=2025-09-07T12:38:21.411+09:00 level=INFO source=memory.go:36 msg=\"new model will fit in available VRAM across minimum required GPUs, loading\" model=models/blobs/sha256-aeda25e63ebd698fab8638ffb778e68bed908b960d39d0becc650fa981609d25 library=cuda parallel=1 required=\"5.3 GiB\" gpus=1\n",
      "time=2025-09-07T12:38:21.413+09:00 level=INFO source=server.go:543 msg=offload library=cuda layers.requested=-1 layers.model=35 layers.offload=35 layers.split=[35] memory.available=\"[39.1 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.3 GiB\" memory.required.partial=\"5.3 GiB\" memory.required.kv=\"214.0 MiB\" memory.required.allocations=\"[5.3 GiB]\" memory.weights.total=\"2.3 GiB\" memory.weights.repeating=\"1.8 GiB\" memory.weights.nonrepeating=\"525.0 MiB\" memory.graph.full=\"517.0 MiB\" memory.graph.partial=\"1.0 GiB\" projector.weights=\"795.9 MiB\" projector.graph=\"1.0 GiB\"\n",
      "time=2025-09-07T12:38:21.415+09:00 level=INFO source=runner.go:1170 msg=load request=\"{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:2048 KvCacheType: NumThreads:40 GPULayers:35[ID:GPU-3e598db6-9daf-4004-6afe-71cd19c1e433 Layers:35(0..34)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}\"\n",
      "time=2025-09-07T12:38:21.574+09:00 level=INFO source=ggml.go:131 msg=\"\" architecture=gemma3 file_type=Q4_K_M name=\"\" description=\"\" num_tensors=883 num_key_values=36\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-PCIE-40GB, compute capability 8.0, VMM: yes, ID: GPU-3e598db6-9daf-4004-6afe-71cd19c1e433\n",
      "load_backend: loaded CUDA backend from /usr/local/lib/ollama/libggml-cuda.so\n",
      "load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-skylakex.so\n",
      "time=2025-09-07T12:38:21.697+09:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n",
      "time=2025-09-07T12:38:22.329+09:00 level=INFO source=ggml.go:487 msg=\"offloading 34 repeating layers to GPU\"\n",
      "time=2025-09-07T12:38:22.329+09:00 level=INFO source=ggml.go:493 msg=\"offloading output layer to GPU\"\n",
      "time=2025-09-07T12:38:22.329+09:00 level=INFO source=ggml.go:498 msg=\"offloaded 35/35 layers to GPU\"\n",
      "time=2025-09-07T12:38:22.329+09:00 level=INFO source=backend.go:310 msg=\"model weights\" device=CUDA0 size=\"3.1 GiB\"\n",
      "time=2025-09-07T12:38:22.329+09:00 level=INFO source=backend.go:315 msg=\"model weights\" device=CPU size=\"525.0 MiB\"\n",
      "time=2025-09-07T12:38:22.329+09:00 level=INFO source=backend.go:321 msg=\"kv cache\" device=CUDA0 size=\"214.0 MiB\"\n",
      "time=2025-09-07T12:38:22.329+09:00 level=INFO source=backend.go:332 msg=\"compute graph\" device=CUDA0 size=\"1.1 GiB\"\n",
      "time=2025-09-07T12:38:22.329+09:00 level=INFO source=backend.go:337 msg=\"compute graph\" device=CPU size=\"5.0 MiB\"\n",
      "time=2025-09-07T12:38:22.329+09:00 level=INFO source=backend.go:342 msg=\"total memory\" size=\"4.9 GiB\"\n",
      "time=2025-09-07T12:38:22.330+09:00 level=INFO source=sched.go:473 msg=\"loaded runners\" count=1\n",
      "time=2025-09-07T12:38:22.330+09:00 level=INFO source=server.go:1250 msg=\"waiting for llama runner to start responding\"\n",
      "time=2025-09-07T12:38:22.333+09:00 level=INFO source=server.go:1284 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
      "time=2025-09-07T12:38:24.090+09:00 level=INFO source=server.go:1288 msg=\"llama runner started in 2.97 seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:38:35 | 200 | 15.831757181s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "장곡사 미륵불 괘불탱은 조선 후기 대표적인 불화 작품 중 하나로, 한국 불교 미술의 뛰어남을 보여주는 중요한 유물입니다. 다음은 장곡사 미륵불 괘불탱에 대한 자세한 설명입니다.\n",
      "\n",
      "**1. 괘불탱이란?**\n",
      "\n",
      "*   **괘(掛)**: 널브러뜨린 둥근 천을 의미합니다.\n",
      "*   **불탱(佛像)**: 불상(佛像)을 그림으로 표현한 것을 의미합니다.\n",
      "*   **괘불탱**은 괘를 배경으로 불상을 그린 널브러뜨린 형태의 불화입니다. \n",
      "*   일반적으로 괘불탱은 좌선(坐禪) 자세를 한 불상을 널브러뜨려 그리는 방식으로, 불교 신자들이 정제된 마음으로 불상을 바라보고 예배하는 모습을 상징합니다.\n",
      "\n",
      "**2. 장곡사 미륵불 괘불탱의 특징**\n",
      "\n",
      "*   **크기:** 높이 7.8m, 가로 13.8m, 세로 3.4m의 거대한 규모를 자랑합니다.\n",
      "*   **화기(筆法):** 조선 후기 대표적인 화풍인 ‘청화(淸畵)’ 기법이 사용되었습니다. 깨끗하고 맑은 느낌을 주며, 섬세하고 정교한 묘사가 돋보입니다.\n",
      "*   **색채:** 붉은색, 금색, 흰색 등 강렬하고 밝은 색채를 사용하여 불상의 위엄과 신성함을 강조했습니다.\n",
      "*   **미륵불의 모습:** 미륵불은 현생불로서, 석가모니 이전의 현현으로 여겨졌습니다. 괘불탱의 미륵불은 수수색의 옷을 입고, 지나상(指經) 자세로, 손으로 경전을 들고 있는 모습으로 표현되어 지혜와 수행의 의미를 담고 있습니다.\n",
      "*   **장식:** 괘불탱에는 다양한 장식 요소들이 사용되었습니다. \n",
      "    *   **보주(寶珠):** 미륵불의 머리에 있는 보주를 통해 지혜와 깨달음을 상징합니다.\n",
      "    *   **수의(垂衣):** 미륵불의 옷을 흘러내리는 모습은 인간의 욕망을 초월하여 깨달음을 얻고자 하는 염원을 나타냅니다.\n",
      "    *   **장미( garlands):** 괘의 양쪽에 장미를 드리워 화려함을 더했습니다.\n",
      "    *   **보살:** 괘의 양쪽에 보살을 배치하여 미륵불을 보호하고 지지하는 모습을 표현했습니다.\n",
      "\n",
      "**3. 제작 배경 및 역사**\n",
      "\n",
      "*   장곡사 미륵불 괘불탱은 1769년(고종 성종 18년)에 제작되었습니다.\n",
      "*   당시 장곡사는 조선 후기 대표적인 사찰 중 하나였으며, 미륵불 괘불탱은 장곡사의 불교적 위상을 높이는 데 중요한 역할을 했습니다.\n",
      "*   1967년 장곡사가 폐쇄되면서 괘불탱은 장곡사에서 분리되어 현재의 위치에 보관되었습니다.\n",
      "\n",
      "**4. 현재 위치**\n",
      "\n",
      "*   장곡사 미륵불 괘불탱은 현재 **전주비전문화공원**에 보관되어 있습니다.\n",
      "\n",
      "**5. 참고 자료**\n",
      "\n",
      "*   **전주비전문화공원 홈페이지:** [https://www.jeonbubijon.or.kr/](https://www.jeonbubijon.or.kr/)\n",
      "*   **나무위키:** [https://namu.wiki/w/%EC%98%A4%EC%98%A4%EC%98%81%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80](https://namu.wiki/w/%EC%98%A4%EC%98%A4%EC%98%81%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80%EC%9D%B4%EC%A7%80)\n",
      "\n",
      "이 설명이 장곡사 미륵불 괘불탱에 대한 이해를 돕는 데 도움이 되었기를 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "# 단일 메시지\n",
    "resp = llm1.invoke(\"장곡사 미륵불 괘불탱에 대해서 설명해줘.\")\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2025/09/07 - 12:39:38 | 200 |   7.44814141s |       127.0.0.1 | POST     \"/api/chat\"\n",
      "안녕하세요. 저는 AI 문화유산 스마트도슨트입니다. 장곡사 미륵불 괘불탱에 대해 자세히 설명해 드리겠습니다. \n",
      "\n",
      "**장곡사 미륵불 괘불탱이란?**\n",
      "\n",
      "장곡사 미륵불 괘불탱은 조선 후기 대표적인 불화로, 1764년에 제작된 괘상(掛像)입니다. 괘상은 널빤지 위에 괘를 덧댄 형태로, 불상을 얹어놓고 훼손을 막는 역할을 합니다. 이 괘불탱은 특히 뛰어난 화제성과 제작 기술, 그리고 역사적 의미를 지니고 있어 한국 불화의 중요한 유산으로 평가받고 있습니다.\n",
      "\n",
      "**괘불탱의 특징**\n",
      "\n",
      "*   **크기:** 높이 7.8m, 가로 13.7m의 거대한 크기를 자랑하며, 당시 제작 기술의 수준을 보여줍니다.\n",
      "*   **화제성:** 미륵불은 석가모니 이전의 불상으로, 미래의 석가모니를 상징하는 존재입니다. 장곡사 미륵불 괘불탱은 미륵불의 신성함과 미래에 대한 희망을 담고 있어 당시 사회에 큰 영향을 미쳤습니다.\n",
      "*   **제작 기술:** 괘상 제작은 매우 정교하고 숙련된 기술을 필요로 합니다. 괘상 제작에는 옻칠, 황금, 다양한 색상의 물감 등 고가의 재료가 사용되었으며, 제작 과정 또한 여러 단계의 과정을 거쳐 완성되었습니다.\n",
      "*   **화제:** 괘상 위에 괘를 덧댄 형태는 불상을 보호하고 훼손을 막는 역할을 합니다. 또한, 괘상 위에 그려진 화제는 불상의 신성함을 강조하고, 불상의 모습을 더욱 돋보이게 합니다.\n",
      "*   **역사적 의미:** 장곡사는 조선 초기부터 사찰로 존립해 온 곳으로, 장곡사 미륵불 괘불탱은 장곡사의 역사와 문화를 담고 있습니다.\n",
      "\n",
      "**괘불탱의 내용**\n",
      "\n",
      "*   **미륵불의 모습:** 괘불탱의 중심에는 미륵불이 묘형을 하고 있습니다. 미륵불은 수수께끼 같은 미소를 짓고 있으며, 손에는 서원지를 들고 있습니다. 서원지는 미륵불의 구원을 담은 inscription입니다.\n",
      "*   **화려한 색채:** 괘불탱은 다양한 색채를 사용하여 제작되었습니다. 특히 황금은 미륵불의 신성함을 강조하는 데 사용되었으며, 다른 색채들은 불상의 모습을 더욱 돋보이게 하는 데 사용되었습니다.\n",
      "*   **화제:** 괘상 위에는 괘를 덧댄 형태로, 불상의 신성함을 강조하는 데 사용되었습니다.\n",
      "\n",
      "**장곡사 미륵불 괘불탱의 현재**\n",
      "\n",
      "장곡사 미륵불 괘불탱은 1969년 국보로 지정되었으며, 현재 장곡사 대웅전에 보관되어 신도들의 예배와 참배를 위해 사용되고 있습니다. \n",
      "\n",
      "더 궁금한 점이 있으시면 언제든지 질문해주세요.\n"
     ]
    }
   ],
   "source": [
    "resp = llm1.invoke([\n",
    "    SystemMessage(content=\"너는 AI 문화유산 스마트도슨트야.\"),\n",
    "    HumanMessage(content=\"장곡사 미륵불 괘불탱에 대해서 설명해줘.\"),\n",
    "])\n",
    "print(resp.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
